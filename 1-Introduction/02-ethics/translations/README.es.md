# Introducci√≥n a la √©tica de datos

|![ Sketchnote por [(@sketchthedocs)](https://sketchthedocs.dev) ](../../sketchnotes/02-Ethics.png)|
|:---:|
| √âtica para ciencia de datos - _Sketchnote por [@nitya](https://twitter.com/nitya)_ |

---

Todos somos ciudadanos de datos viviendo en un gobernado por datos.

Las tendencias de mercado nos dicen que para 2022, 1 de cada 3 grandes organizaciones comprar√° y vender√° sus datos en l√≠nea a trav√©s de [mercados e intercambios](https://www.gartner.com/smarterwithgartner/gartner-top-10-trends-in-data-and-analytics-for-2020/). Como **desarrolladores de Apps**, lo encontraremos m√°s f√°cil y barato que integrar conocimientos dirigidos por datos y automatizaci√≥n dirigida por algoritmos en las experiencias de usuario del d√≠a a d√≠a. Pero como la AI se vuelve cada vez m√°s presente, necesitarempos entender lso da√±os potenciales causados por el [armamentismo](https://www.youtube.com/watch?v=TQHs8SA1qpk) de dichos algoritmos a escala.

Las tendencias tambi√©n indican que crearemos y consumiremos m√°s de [180 zettabytes](https://www.statista.com/statistics/871513/worldwide-data-created/) de datos para el 2025. Como **cient√≠ficos de datos**, esto nos da niveles de acceso sin precedentes a datos personales. Esto significa que podemos construir perfiles conductuales de usuarios e influenciar en la toma de decisiones en formas que crea una [ilusi√≥n de libre elecci√≥n](https://www.datasciencecentral.com/profiles/blogs/the-illusion-of-choice) mientras empuja a los usuarios hacia los resultados que preferimos. Tambi√©n plantea preguntas m√°s amplias respecto a la privacidad de datos y protecci√≥n de los usuarios.

La √©tica de datos son _barreras de seguridad necesarias_ para la ciencia de datos e ingenier√≠a, ayud√°ndonos a minimizar da√±os potenciales y consecuencias no deseadas de nuestras acciones dirigidas por datos. El [Hype Cycle de Gartner para AI](https://www.gartner.com/smarterwithgartner/2-megatrends-dominate-the-gartner-hype-cycle-for-artificial-intelligence-2020/) identifica tendencias relevantes en √©tica digital, AI responsable, y gobernanza de AI como factores clave para mega-tendencias mayores alrededor de la _democratizaci√≥n_ e _industrializaci√≥n_ de la AI.

![Hype Cycle de Gartner para AI - 2020](https://images-cdn.newscred.com/Zz1mOWJhNzlkNDA2ZTMxMWViYjRiOGFiM2IyMjQ1YmMwZQ==)

En esta lecci√≥n, exploraremos la fascinante √°rea de la √©tica de datos - desde los conceptos clave y desaf√≠os, hasta los casos de estudio y conceptos de AI aplicados como gobernanza - que ayuda a establecer una cultura de √©tica en equipos y organizaciones que trabajan con datos y AI.




## [Examen previo a la lecci√≥n](https://red-water-0103e7a0f.azurestaticapps.net/quiz/2) üéØ

## Definiciones b√°sicas

Empecemos entendiendo la terminolog√≠a b√°sica.

La palabra "√©tica" proviene de la [palabra griega "ethikos"](https://en.wikipedia.org/wiki/Ethics) (y su ra√≠z "ethos") lo cual significa _car√°cter o naturaleza moral_. 

La **√©tica** se trata de valores compartidos y principios morales que gobiernan nuestro comportamiento en sociedad. La √©tica se basa no en leyes sino en normas m√°s ampliamente aceptadas de lo que es "correcto vs lo incorrecto". Sin embargo, las consideraciones √©ticas pueden influenciar iniciativas de gobernanza corporativa y regulaciones de gobernanza que crean m√°s incentivos para el cumplimiento.

La **√©tica de datos** es una [nueva rama de la √©tica](https://royalsocietypublishing.org/doi/full/10.1098/rsta.2016.0360#sec-1) que "estudia y evalua problemas morales relacionados a _datos, algoritmos y pr√°cticas correspondientes_"- Aqu√≠, los **"datos"** se centran en acciones relacionadas a la generaci√≥n, grabaci√≥n, curaci√≥n, procesamiento de difusi√≥n, intercambio y uso de **"algoritmos"** centrados en AI, agentes, aprendizaje autom√°tico, y robots, as√≠ como **"pr√°cticas"** enfocadas en temas como inovaci√≥n responsable, programaci√≥n, hackeo y c√≥digos de √©tica.

**√âtica aplicada** es la [aplicaci√≥n pr√°ctica de consideraciones morales](https://en.wikipedia.org/wiki/Applied_ethics). Es el proceso de investigar activamente cuestiones √©ticas en el contexto de _acciones del mundo real, productos y procesos_, y tomar medidas correctivas para hacer que estos se alinean con nuestros valores √©ticos definidos.

**Cultura √©tica** trata de [_operacionalizar_ la √©tica aplicada](https://hbr.org/2019/05/how-to-design-an-ethical-organization) para confirmar que nuestros principios √©ticos y pr√°cticas son adoptados de forma consistente y escalable a trav√©s de toda la organizaci√≥n. Una cultura √©tica exitosa define principios √©ticos a nivel organizaci√≥n, provee incentivos significativos para el cumplimiento y refuerza las normas √©ticas alentando y amplificando los comportamientos deseados en cada nivel de la organizaci√≥n.


## Conceptos √©ticos

En esta secci√≥n, duscutiremos conceptos como **valores compartidos** (principios) y **retos √©ticos** (problemas) para la √©tica de datos - y explora **casos de estudio** que te ayudan a entender estos conceptos en el contexto del mundo real.

### 1. Principios √©ticos

Cada estrategia de √©tica de datos comienza por la definici√≥n de _principios √©ticos_ - los "valores compartidos" que describen los comportamientos aceptables, y gu√≠an acciones de conformidad, en nuestros proyectos de datos y AI. Puedes definir estos a nivel individual o de equipo. Sin embargo, la mayor√≠a de las grandes organizaciones describen estos en una misi√≥n _√©tica de AI_ o marco de trabajo que es definido a niveles corporativos y refuerza consistentement a trav√©s de todos los equipos.

**Ejemplo:** La misi√≥n [responsable de AI](https://www.microsoft.com/en-us/ai/responsible-ai) de Microsoft se lee: _"Estamos comprometidos al avance de principios √©ticos dirigidos por AI que anteponen primero a la gente"_ - identificando 6 principios √©ticos en el marco de trabajo descrito a continuaci√≥n:

![AI responsable en Microsoft](https://docs.microsoft.com/en-gb/azure/cognitive-services/personalizer/media/ethics-and-responsible-use/ai-values-future-computed.png)

Exploremos brevemente estos principios. La _transparencia_ y _responsabilidad_ son los valores fundamentales sobre los que se cimientan otros principios - iniciemos aqu√≠:

* [**Responsabilidad**](https://www.microsoft.com/en-us/ai/responsible-ai?activetab=pivot1:primaryr6) hace a los practicantes _responsables_ por sus datos y operaciones de AI, en cumplimiento con estos principios √©ticos.
* [**Transparencia**](https://www.microsoft.com/en-us/ai/responsible-ai?activetab=pivot1:primaryr6) asegura que los datos y acciones de AI sean _entendibles_ (interpretables) para los usuarios, explicando el qu√© y el porqu√© detr√°s de las decisiones.
* [**Justicia**](https://www.microsoft.com/en-us/ai/responsible-ai?activetab=pivot1%3aprimaryr6) - se centra en asegurar que la AI trata a _todas las personas_ justamente, dirigiendo cualquier sesgo sist√©mico o social-√©tico impl√≠cito in datos y sistemas.
* [**Fiabilidad y seguridad**](https://www.microsoft.com/en-us/ai/responsible-ai?activetab=pivot1:primaryr6) - asegura que la AI se comporta _consistentemente_ con los valores definidos, minimizando da√±os potenciales o consecuencias no intencionadas.
* [**Privacidad & seguridad**](https://www.microsoft.com/en-us/ai/responsible-ai?activetab=pivot1:primaryr6) - se trata del entendimiento del linaje de datos, y provee _privacidad de datos y protecciones relacionadas_ a los usuarios.

* [**Inclusi√≥n**](https://www.microsoft.com/en-us/ai/responsible-ai?activetab=pivot1:primaryr6) - trata del dise√±o de soluciones AI con la intenci√≥n, adapt√°ndolas para reunir un _amplio rango de necesidades humanas_ y capacidades.

> üö® Piensa cual podr√≠a ser tu misi√≥n de √©tica de datos. Explorar marcos de trabajo √©ticos de AI de otras organizaciones - aqu√≠ tienes ejemplos de [IBM](https://www.ibm.com/cloud/learn/ai-ethics), [Google](https://ai.google/principles) , y[Facebook](https://ai.facebook.com/blog/facebooks-five-pillars-of-responsible-ai/). ¬øQu√© valores compartidos tienen en com√∫n? ¬øC√≥mo se relacionan estos principios al producto de AI o industria en la cual operan?

### 2. Desaf√≠os √©ticos

Una vez que tenemos pricipios √©ticos definidos, el siguiente paos es evaluar nuestros datos y acciones AI para ver si estos se alinean con los valores compartidos. Piensa en tus acciones en 2 categor√≠as: _recolecci√≥n de datos_ y _dise√±o de algoritmos_.

Con la recolecci√≥n de datos, las acciones probablemente involucren **datos personales** o informaci√≥n de identificaci√≥n personal (PII) para individuos vivos identificables. Esto incluye [diversos art√≠culos de datos no personales](https://ec.europa.eu/info/law/law-topic/data-protection/reform/what-personal-data_en) que _colectivamente_ identifican a un individuo. Los desaf√≠os √©ticos pueden relacionarse con _privacidad de datos_, _propiedad de los datos_, y temas relacionados como _consentimiento informado_ y _derechos de propiedad intelectual_ para los usuarios.

Con el dise√±o de algoritmo, las acciones involucran la recolecci√≥n y curaci√≥n de **conjuntos de datos**, luego usarlos para entrenar y desplegar **modelos de datos** que predicen resultados o automatizan decisiones en contexto del mundo real. Los desaf√≠os √©ticos pueden surgir de _conjuntos de datos sesgados_, problemas con la _calidad de los datos_, _injusticia_, y _malinterpretaci√≥n_ in los algoritmo - incluyendo algunos problemas que son sist√©micos por naturaleza.

En ambos casos, los desaf√≠os √©ticos destacan √°reas donde nuestas acciones pueden encontrar conflictos con nuestros valores compartidos. Para detectar, mitigar, minimizar, o eliminar estas preocupaciones - necesitamos realizar preguntas morales de "s√≠/no" relacionadas a nuestas acciones, luego tomar acciones correctivas seg√∫n sea necesario. Demos un vistazo a algunos desaf√≠os √©ticos y las preguntas morales que plantean:


#### 2.1 Propiedad de los datos

La recolecci√≥n de datos suele involucrar datos que pueden identificar los sujetos de datos. La [propiedad de los datos](https://permission.io/blog/data-ownership) trata del _control_ y [_derechos de usuario_](https://permission.io/blog/data-ownership) relacionados a la creaci√≥n, procesamiento y dispersi√≥n de los datos.

Las preguntas morales que debemos hacer son:
 * ¬øQui√©n posea los datos? (usuario u organizaci√≥n)
 * ¬øQu√© derechos tienen los sujetos de datos? (ejemplo: acceso, eliminaci√≥n, portabilidad)
 * ¬øQu√© derechos tienen las organizaciones? (ejemplo: rectificar revisiones de usuarios maliciosos)

#### 2.2 Consentimiento informado

[Consentimiento informado](https://legaldictionary.net/informed-consent/) define el acto de los usuarios al aceptar una acci√≥n (como la recolecci√≥n de datos) con un _completo entendimiento_ de hechos relevantes incluyendo el prop√≥sito, riesgos potenciales y alternativas.

Las preguntas a explorar son:
 * ¬øEl usuario (sujeto de datos) otorg√≥ el permiso para el uso y cpatura de datos?
 * ¬øEl usuario entendi√≥ el prop√≥sito para el cual los datos fueron capturados?
 * ¬øEL usuario entendi√≥ los riesgos potenciales de su participaci√≥n?

#### 2.3 Propiedad intelectual

La [propiedad intelectual](https://en.wikipedia.org/wiki/Intellectual_property) se refiere a creaciones intangibles resultado de la iniciativa humana, que puede _tener valor econ√≥mico_ para individuos o negocios.

Las preguntas a explorar son:
 * ¬øLos datos recolectados tiene valor econ√≥mico para un usuario o negocio?
 * ¬øEl **usuario** tiene propiedad intelectual en este √°mbito?
 * ¬øLa **organizaci√≥n** tiene propiedad intelectual en este √°mbito?
 * Si estos derechos existen, ¬øc√≥mo los protegemos?

#### 2.4 Privacidad de datos

La [privacidad de datos](https://www.northeastern.edu/graduate/blog/what-is-data-privacy/) o privacidad de la informaci√≥n se refiere a la preservaci√≥n de la privacidad del usuario y la protecci√≥n de la identidad del usuario respecto a informaci√≥n de identificaci√≥n personal.

Las preguntas a explorar son:
 * ¬øEst√°n los datos (personales) de los usuarios seguros contra hackeos y filtraciones?
 * ¬øEst√°n los datos de usuario accesibles s√≥lo para usuarios y contextos autorizados?
 * ¬øSe preserva el anonimato de los usuarios cuando los datos son compartidos o esparcidos?
 * ¬øPuede un usuario ser desidentificado de conuntos de datos anonimizados?


#### 2.5 Derecho al olvido

El [derecho al olvido](https://en.wikipedia.org/wiki/Right_to_be_forgotten) o [derecho a la eliminaci√≥n](https://www.gdpreu.org/right-to-be-forgotten/) provee protecci√≥n adicional a datos personales de los usuarios. Especialmente, brinda a los usuarios el derecho a solicitar la eliminaci√≥n o remoci√≥n de datos personales de b√∫squedas de internet y otras ubicaciones, _bajo circunstancias espec√≠ficas_ - permiti√©ndoles un nuevo comienzo en l√≠nea sin las acciones pasadas siendo retenidas contra √©l.

Las preguntas a explorar son:
 * ¬øEl sistema permite a los sujetos de datos solicitar eliminaci√≥n?
 * ¬øLa remoci√≥n del consentimiento del usuario deber√≠a disparar la eliminaci√≥n automatizada?
 * ¬øSe recolectaron los datos sin consentimiento o por medios no leg√≠timos?
 * ¬øEstamos de acuerdo con las regulaciones de gobierno para la privacidad de los datos?


#### 2.6 Sesgo del conjunto de datos

Un conjunto de datos o [sesgo de recopilaci√≥n](http://researcharticles.com/index.php/bias-in-data-collection-in-research/) pretende seleccionar un subconjunto _no representativo_ de datos para el desarrollo de un algor√≠tmo, creando una potencial injusticia en los resultados para distintos grupos. Los tipos de sesgos incluyen selecci√≥n o muestreo de sesgo, sesgo voluntario y sesgo de instrumento.

Las preguntas a explorar son:
 * ¬øReclutamos un conjunto representativo de sujetos de datos?
 * ¬øProbamos nuestros conjuntos de datos recoletados o curados para distintos sesgos?
 * ¬øPodemos mitigar o eliminar los sesgos descubiertos?

#### 2.7 Calidad de los datos

[La calidad de los datos](https://lakefs.io/data-quality-testing/) se enfoca en la validez de los conjuntos de datos curados que se usan para desarrollar nuestros algoritmos, comprobando si las caracter√≠sticas y registros cumplen los requerimientos para el nivel de precisi√≥n necesario para nuestros prop√≥sitos de AI.

Las preguntas a explorar son:
 * ¬øCapturamos _caracter√≠sticas_ v√°lidas para nuestro caso de uso?
 * ¬øLos datos fueron capturados _consistentemente_ a trav√©s de las distintas fuentes de datos?
 * ¬øEst√°n _completos_ los conjuntos de datos para las distintas condiciones o escenarios?
 * ¬øLa informaci√≥n es capturada de forma _precisa_ reflejando la realidad?

#### 2.8 Justicia del algoritmo

[La justicia del algoritmo](https://towardsdatascience.com/what-is-algorithm-fairness-3182e161cf9f) verifica que el dise√±o del algoritmo discrimina sistem√°ticamente contra subgrupos espec√≠ficos de sujetos de datos que conlleven a [da√±os potenciales](https://docs.microsoft.com/en-us/azure/machine-learning/concept-fairness-ml) en _asignaci√≥n_, (donde los recursos son negados o retenidos para ese grupo) y _calidad del servicio_ (donde la AI no es tan precisa para algunos subgrupos como lo es para otros).

Las preguntas a explorar son:
 * ¬øEvaluamos la precisi√≥n del modelo para distintos subgrupos y condiciones?
 * ¬øEscrutinamos el sistema buscando da√±os potenciales (por ejemplo, estereotipos?
 * ¬øPodemos revisar los datos o retener re-entrenar modelos para mitigar da√±os potenciales?

Explora recursos como [Listas de comprogaci√≥n de justicia de AI](https://query.prod.cms.rt.microsoft.com/cms/api/am/binary/RE4t6dA) para aprender m√°s.

#### 2.9 Malinterpretaci√≥n

[La malinterpretaci√≥n de datos](https://www.sciencedirect.com/topics/computer-science/misrepresentation) trata de preguntarse si estamos comunicando ideas honestamente de los datos reportados en una forma enga√±osa para soportar la narrativa deseada.

Las preguntas a explorar son:
 * ¬øEstamos reportando datos incompletos o inexactos?
 * ¬øEstamos visualizando los datos de tal forma conllevan a conclusiones enga√±osas?
 * ¬øEstamos usando t√©cnicas estad√≠sticas selectivas para manipular los resultados?
 * ¬øExisten explicaciones alternativas par pueden ofrecer una conclusi√≥n distinta?

#### 2.10 Libertad de elecci√≥n
La [ilusi√≥n de libertad de elecci√≥n](https://www.datasciencecentral.com/profiles/blogs/the-illusion-of-choice) ocurre cuando un sistema "elige arquitecturas" usando algoritmos de toma de decisiones para empujar a la gente hacia la elecci√≥n de un resultado preferido mientras aparenta darles opciones y control. Estos [patrones obscuros](https://www.darkpatterns.org/) pueden causar da√±o social y econ√≥mico a los usuarios. Ya que las decisiones del usuario impactan en los perfiles de comportamiento, estas acciones dirigen potencialmente a futuras elecciones que pueden amplificar y extender el impacto de estos da√±os.

Las preguntas a explorar son:
 * ¬øEl usuario entendi√≥ las implicaciones de realizar dicha elecci√≥n?
 * ¬øEl usuario estaba conciente de las opciones (alternativas) y los pros y contrar de cada una?
 * ¬øEl usuario puede revertir una elecci√≥n influenciada o automatizada posteriormente?

### 3. Casos de estudio

Para poner estos desaf√≠os √©ticos en contexto del mundo real, ayuda ver casos de estudio que destacan el da√±o potencial y las consecuencias a individuos y sociedad, cuando dichas violaciones √©ticas son pasadas por alto.

Aqu√≠ hay algunos ejemplos:

| Desaf√≠o de √©tica | Caso de estudio  | 
|--- |--- |
| **Consentimiento informado** | 1972 - [Estudio de s√≠filis Tuskegee](https://en.wikipedia.org/wiki/Tuskegee_Syphilis_Study) - A los hombres afroamericanos que participaron en el estudio le fue prometido tratamiento m√©dico gratuito _pero fueron enga√±ados_ por los investigadores quienes fallaron al informar a los sujetos en sus diagn√≥sticos o en la disponibilidad del tratamiento. Muchos sujetos murieron y los compa√±eros o hijos fueron afectados; el estudio dur√≥ 40 a√±os. | 
| **Privacidad de los datos** |  2007 - El [premio de datos de Netflix](https://www.wired.com/2007/12/why-anonymous-data-sometimes-isnt/) otorg√≥ a investigadores con _10M de clasificaciones an√≥minas de 50K clientes_ para ayudar a mejorar los algoritmos de recomendaci√≥n. Sin embargo, los investigadores fueron capaces de correlacionar datos an√≥nimos con datos personalmente identificables en _conjuntos de datos externos_ (por ejemplo, comentarios en IMDb) - efectivamente "des-anonimizando" a algunos subscriptores de Netflix.|
| **Sesgo de colecci√≥n**  | 2013 - La ciudad de Boston [desarroll√≥ Street Bump](https://www.boston.gov/transportation/street-bump), una app que permite a los ciudadanos reportar baches, dando a la ciudad mejores datos de la carretera para encontrar y reparar desperfectos. Sin embargo, [la gente en los grupos con menores ingresos tuvieron menos acceso a autos y tel√©fonos](https://hbr.org/2013/04/the-hidden-biases-in-big-data), haciendo sus problemas de carretera invisibles para la app. Los desarrolladores trabajaron en conjunto con acad√©micos para cambiar _el acceso equitativo y brecha digital_ y as√≠ fuese m√°s justo. |
| **Justicia de algoritmos**  | 2018 - El [estudio de tonos de g√©nero](http://gendershades.org/overview.html) del MIT evalu√≥ la precisi√≥n de productos de clasificaci√≥n de g√©nero , exponiendo brechas en la precisi√≥n para mujeres y personas de color. Una [tarjeta 2019 de Apple](https://www.wired.com/story/the-apple-card-didnt-see-genderand-thats-the-problem/) parec√≠a ofrecer menos cr√©dit a mujeres que a hombres. Ambos ilustraron problemas en sesgos de algoritmos llevando a da√±os socio-econ√≥micos.
| **Malinterpretaci√≥n de datos** | 2020 - El [departamento de salud p√∫blica de Georgia liber√≥ gr√°ficos de COVID-19](https://www.vox.com/covid-19-coronavirus-us-response-trump/2020/5/18/21262265/georgia-covid-19-cases-declining-reopening) que parec√≠an malinformar a los ciudadanos acerca de las tendencias en los casos confirmados sin orden cronol√≥gico en el eje x. Esto ilustra la malinterpretaci√≥n a trav√©s de visualizaciones enga√±osas. |
| **Ilusi√≥n de libertad de elecci√≥n** | 2020 - La aplicaci√≥n de aprendizaje [ABCmouse pag√≥ $10M para asentar una queja FTC](https://www.washingtonpost.com/business/2020/09/04/abcmouse-10-million-ftc-settlement/) donde los padres fueron enga√±ados para pagar subscripciones que no pod√≠an cancelar. Esto ilustra los patrones obscuros en arquitecturas de elecci√≥n, donde los usuarios fueron empujados hacia elecciones potencialmente da√±inas. |
| **Privacidad de los datos y derechos de usuario** | 2021 - La [infracci√≥n de datos](https://www.npr.org/2021/04/09/986005820/after-data-breach-exposes-530-million-facebook-says-it-will-not-notify-users) de Facebook expuso datos de 530M de usuarios, resultando en un acuerdo de $5B para la FTC. Sin embargo, esto rechaz√≥ notificar a los usuarios de la brecha violando los derechos de usuarios alrededor de la transparencia y acceso de datos. |

¬øQuieres explorar m√°s casos de estudio? Revisa los siguientes recursos:
* [√âtica desenvuelta](https://ethicsunwrapped.utexas.edu/case-studies) - dilemas √©ticos en diversas industrias.
* [Curso de √©tica en ciencia de datos](https://www.coursera.org/learn/data-science-ethics#syllabus) - referenc√≠a los casos de estudio explorados.
* [Donde las cosas han ido mal](https://deon.drivendata.org/examples/) - lista de comprobaci√≥n de deon con ejemplos

> üö® Piensa en los casos de estudio que has visto - ¬øhas experimentado o sido afectado por un desaf√≠o √©tico similar en tu vida? ¬øPuedes pensar en al menos otro caso de estudio que ilustre uno de los desaf√≠os √©ticos que discutimos en esta secci√≥n?

## √âtica aplicada

Hemos hablado de conceptos √©ticos, desaf√≠os y casos de estudio en contextos del mundo real. Pero ¬øc√≥mo podemos _aplicar_ los principios √©ticos y pr√°cticas en nuestros proyectos? y ¬øc√≥mo _aplicamos_ estas pr√°cticas para una mejor gobernanza? Exploremos algunas soluciones del mundo real:

### 1. C√≥digos profesionales

Los c√≥digos profesionales ofrecen una opci√≥n para que las organizaciones "incentiven" a los miembros a apoyar sus principios √©ticos y su misi√≥n. Los c√≥digos son _gu√≠as morales_ para el comportamiento profesional, que ayudan a los empleados o miembros a tomar decisiones que se alinea con sus principios de organizaci√≥n. Estas son tan buenas como el cumplimiento voluntario de los miembros; sin embargo, muchas organizaciones ofrecen incentivos adicionales y penalizaciones para motivar el cumplimiento de los miembros.

Los ejemplos incluyen:

 * C√≥digo de √©tica de [Oxford Munich](http://www.code-of-ethics.org/code-of-conduct/)
 * C√≥digo de conducta  de la [Asociaci√≥n de ciencia de datos](http://datascienceassn.org/code-of-conduct.html) (creado en 2013)
 * [C√≥digo de √©tica y conducta profesional de ACM](https://www.acm.org/code-of-ethics) (desde 1993)

> üö® ¬øPerteneces a una organizaci√≥n profesional de ingenier√≠a o ciencia de datos? Explora su sitio para ver si definen un c√≥digo de √©tica profesional. ¬øQu√© te dice acerca de sus principios √©ticos? ¬øC√≥mo "incentivan" a los miembros para que sigan el c√≥digo?

### 2. Listas de comprobaci√≥n de √©tica

Mientras los c√≥digos profesionales defiene los _comportamientos √©ticos_ requerido por sus practicantes, estos tienen [limitaciones conocidas](https://resources.oreilly.com/examples/0636920203964/blob/master/of_oaths_and_checklists.md) en su aplicaci√≥n, particularmente en proyectos a gran escala. En su lugar, muchos expertos en ciencia de datos [abogan por listas de comprobaci√≥n](https://resources.oreilly.com/examples/0636920203964/blob/master/of_oaths_and_checklists.md), que pueden **conectar principios a pr√°cticas** en formas m√°s determin√≠sticas y accionables.

Las listas de comprobaci√≥n convierten preguntas en tareas de "s√≠/no" que pueden ser operadas, permitiendo darles seguimiento como parte de flujos de trabajo de liberaci√≥n de productos est√°ndar.

Los ejemplos incluyen:
 * [Deon](https://deon.drivendata.org/) - una lista de comprobaci√≥n de √©tica de datos de prop√≥sito general creada a partir de [recomendaciones de la industria](https://deon.drivendata.org/#checklist-citations) con una herramienta de l√≠nea de comandos para su f√°cil integraci√≥n.
 * [Lista de comprobaci√≥n de auditor√≠a de privacidad](https://cyber.harvard.edu/ecommerce/privacyaudit.html) - provee orientaci√≥n general para pr√°cticas de manejo de la informaci√≥n desde perspectivas legales y sociales.
 * [Lista de comprobaci√≥n de justicia de AI](https://www.microsoft.com/en-us/research/project/ai-fairness-checklist/) - creada por practicantes de AI para soportar la adopci√≥n e integraci√≥n de controles justos en los ciclos de desarrollo de AI.
 * [22 preguntas para √©tica en datos y AI](https://medium.com/the-organization/22-questions-for-ethics-in-data-and-ai-efb68fd19429) - marcos de trabajo m√°s abiertos, estructurados para la exploraci√≥n inicial de problemas √©ticos en contextos de dise√±o, implementaci√≥n y organizaci√≥n.

### 3. Regulaciones √©ticas

La √©tica trata de definir valores compartidos y hacer lo correcto _voluntariamente_. El **cumplimiento** trata de _seguir la ley_ donde se define. La **gobernanza** cubre ampliamente todas las formas en las cuales las organizaciones operan para hacer cumplir los principios √©ticos y seguir las leyes establecidas.

Hoy en d√≠a, la gobernanza toma dos formas dentro de la organizaci√≥n. Primero, define los principios **√©ticos de AI** y establece pr√°cticas para promover la adopci√≥n en todos los proyectos relacionados a AI en la organizaci√≥n. Segundo, trata de cumplir con todoso los mandatos de gobierno en **regulaciones de protecci√≥n de datos** para las regiones en las cuales opera.

Ejemplos de protecci√≥n de datos y regulaciones de privacidad:

 * `1974`, [Ley de privacidad de EE.UU.](https://www.justice.gov/opcl/privacy-act-1974) - regula al  _gobierno federal_ la recolecci√≥n, uso y divulgaci√≥n de informaci√≥n personal.
 * `1996`, [Ley de responsabilidad y portabilidad de seguro de salud de EE.UU. (HIPAA)](https://www.cdc.gov/phlp/publications/topic/hipaa.html) - protege los datos de salud personales.
 * `1998`, [Ley de protecci√≥n de la privacidad en l√≠nea para ni√±os de EE.UU. (COPPA)](https://www.ftc.gov/enforcement/rules/rulemaking-regulatory-reform-proceedings/childrens-online-privacy-protection-rule) - protege la privacidad de los datos para menores de 13 a√±os.
 * `2018`, [Regulaci√≥n de protecci√≥n general de los datos (GDPR)](https://gdpr-info.eu/) - provee derechos de usuario, protecci√≥n de datos y privacidad.
 * `2018`, [Ley de privacidad para los consumidores de California (CCPA)](https://www.oag.ca.gov/privacy/ccpa) da a los consumidores m√°s _derechos_ sobre sus datos (personales).
 * `2021`, [Ley China de protecci√≥n de la informaci√≥n personal](https://www.reuters.com/world/china/china-passes-new-personal-data-privacy-law-take-effect-nov-1-2021-08-20/) reci√©n establecida, crea una de las regulaciones m√°s grandes a nivel mundial respecto a privacidad de los datos.

> üö® La Uni√≥n Europea defini√≥ la GDPR (regulaci√≥n general de protecci√≥n de datos) quedando como una de las regulaciones a la privacidad de los datos m√°s influyentes de hoy en d√≠a. ¬øSab√≠as que tambi√©n define [8 derechos de usuario](https://www.freeprivacypolicy.com/blog/8-user-rights-gdpr) para la protecci√≥n de la privacidad digital de los ciudadanos y datos personales? Aprende m√°s acerca de qu√© son y porqu√© importan.


### 4. Cultura √©tica

Nota que existe una brecha intangible entre _cumplimiento_ (hacer suficiente para cumplir "lo designado por ley") y atender [problemas sist√©micos](https://www.coursera.org/learn/data-science-ethics/home/week/4) (como la osificaci√≥n, asimetr√≠a de la informaci√≥n e injusticia distribucional) que acelera el armamento de la AI.

Lo √∫ltimo requier [enfoques colaborativos para definir culturas de √©tica](https://towardsdatascience.com/why-ai-ethics-requires-a-culture-driven-approach-26f451afa29f) que construyan conexiones emocionales y valores compartidos consistentes _a trav√©s de las organizaciones_ en la industria. Esto hace un llamado a [culturas de √©tica de datos m√°s formalizadas](https://www.codeforamerica.org/news/formalizing-an-ethical-data-culture/) en las organizaciones - permitiendo a _cualquiera_ tirar del [cord√≥n de Andon](https://en.wikipedia.org/wiki/Andon_(manufacturing)) (para plantear cuestiones √©ticas desde el principio en el proceso) y hacer de las _evaluaciones √©ticas_ (por ejemplo, en la contrataci√≥n) un criterio principal en la formaci√≥n de equipos en proyectos de AI. 

---
## [Examen posterior a la lecci√≥n](https://red-water-0103e7a0f.azurestaticapps.net/quiz/3) üéØ
## Revisi√≥n y auto-estudio

Los siguientes cursos y libros te facilitar√°n el entendimiento de conceptos √©ticos principales y desaf√≠os, mientras que los casos de estudio y herramientas te ayudar√°n con las pr√°cticas √©ticas aplicadas en contextos del mundo real. Aqu√≠ tienes algunos recursos con los que comenzar.

* [Aprendizaje autom√°tico para principiantes](https://github.com/microsoft/ML-For-Beginners/blob/main/1-Introduction/3-fairness/README.md) - lecciones de justicia, de Microsoft.
* [Principios de AI responsable](https://docs.microsoft.com/en-us/learn/modules/responsible-ai-principles/) - ruta de aprendizaje gratuito de Microsoft Learn.
* [√âtica y Ciencia de Datos](https://resources.oreilly.com/examples/0636920203964) - Libro electr√≥nico de O'Reilly (M. Loukides, H. Mason et. al)
* [√âtica de Ciencia de Datos](https://www.coursera.org/learn/data-science-ethics#syllabus) - curso en l√≠nea de la Universidad de Michigan.
* [√âtica desenvuelta](https://ethicsunwrapped.utexas.edu/case-studies) - casos de estudio de la Universidad de Texas.

# Asignaci√≥n 

[Escribe un caso de estudio de √©tica de datos](assignment.md)
